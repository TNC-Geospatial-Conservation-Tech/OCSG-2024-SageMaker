{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56f27871-325c-49ce-bfe4-173a3a13c47b",
   "metadata": {},
   "source": [
    "# Using cloud-naive tools to map invasive species with supervised machine learning and Sentinel 2\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this notebook, we will use existing data of verified land cover and invasive species locations to extract remote sensing data from the European Space Agency satellite Sentinel 2. We will then train a machine learning model to predict invasive plant occurrence, and finally, we will apply this model to Sentinel 2 data acquired at multiple dates to monitor the spread and clearing efforts undertaken as part of TNC's Greater Cape Town water fund."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ad306b-4bd5-485f-8c79-2703ac06efe9",
   "metadata": {},
   "source": [
    "### 1. Load Python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa32c6fe-894d-47ba-b86f-ea4bea85d850",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#core\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import rioxarray as riox\n",
    "import geopandas as gpd\n",
    "import xvec\n",
    "from shapely.geometry import box, mapping\n",
    "#data search\n",
    "import stackstac\n",
    "import pystac_client\n",
    "#plotting\n",
    "import hvplot.xarray\n",
    "import holoviews as hv\n",
    "hvplot.extension('bokeh')\n",
    "#ml\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "#other\n",
    "from dask.diagnostics import ProgressBar\n",
    "\n",
    "#datashader\n",
    "#geoviews"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0dd2f3-91ce-489b-926e-eb0099cd6bec",
   "metadata": {},
   "source": [
    "### 2. Load invasive plant data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79f53c4-d5e2-4315-a7d3-7b84bdcd4803",
   "metadata": {},
   "source": [
    "First we load our land cover and invasive plant location data. We create this in our GIS (Arc or QGIS), using field data and manually inspected high-resolution imagery. \n",
    "\n",
    "We will use the python package `geopandas` to handle spatial vector data. GeoPandas is a Python library designed to handle and analyze geospatial data, similar to how ArcGIS or the sf package in R work. It extends the popular pandas library to support spatial data, allowing you to work with vector data formats like shapefiles, GeoJSON, geopackage and more. GeoPandas integrates well with other Python libraries and lets you can perform spatial operations like overlays, joins, and buffering in a way that's familiar if you're used to sf or ArcGIS workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801050df-39b5-46fc-bde4-4525f9f14d6a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#read data in geopackage with geopandas\n",
    "\n",
    "gdf = gpd.read_file('gctwf_invasive.gpkg')\n",
    "gdf = gdf.to_crs(\"EPSG:32734\")\n",
    "bbox = gdf.total_bounds\n",
    "#interactive plot\n",
    "gdf[['name','geometry']].explore('name',tiles='https://mt1.google.com/vt/lyrs=s&x={x}&y={y}&z={z}', attr='Google')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a710c494-a4e1-4b57-9eae-060f8816755d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#We will need this later to turn codes back to names\n",
    "# Get unique values\n",
    "name_table = gdf[['class', 'name']].drop_duplicates().sort_values(by=['class'])\n",
    "name_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b14016-3a3a-4332-a448-08eb56644b02",
   "metadata": {},
   "source": [
    "### 3. Search for remote sensing data using STAC\n",
    "\n",
    "STAC (SpatioTemporal Asset Catalog) is a standardized specification for describing geospatial datasets and their metadata, making it easier to discover, access, and share spatial-temporal data like satellite imagery, aerial photos, and elevation models. STAC catalogs and APIs provide structured, searchable metadata that allow users to query datasets based on criteria like geographic location, time range, resolution, or cloud coverage.\n",
    "\n",
    "On AWS, STAC datasets can be found in the [Registry of Open Data](https://registry.opendata.aws/), which hosts numerous public geospatial datasets in STAC format. Examples include satellite imagery from Landsat, Sentinel-2, and MODIS. You can use tools like the pystac-client Python library or STAC browser interfaces to explore and retrieve data directly from AWS S3 buckets.\n",
    "\n",
    "Once we have used stac to filter the data we want, we get urls to the location of that data on AWS s3 object storage. If we have our own data direclty stored on s3, we can skip this part and just use that url directly, or we can create our own stac catalog if we have a large collection of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093695b4-e878-4674-b2ed-df5160e807d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "#the location of the catalog we want to search (find this on AWS Registry of Open Data)\n",
    "URL = \"https://earth-search.aws.element84.com/v1\"\n",
    "catalog = pystac_client.Client.open(URL)\n",
    "\n",
    "#we want data that intersect with this location\n",
    "lat, lon = -33.80, 19.20\n",
    "#in this time\n",
    "datefilter = \"2018-09-01/2018-09-30\"\n",
    "\n",
    "#search!\n",
    "items = catalog.search(\n",
    "    intersects=dict(type=\"Point\", coordinates=[lon, lat]),\n",
    "    collections=[\"sentinel-2-l2a\"],\n",
    "    datetime=datefilter,\n",
    "    query={\"eo:cloud_cover\": {\"lt\": 10}}  # Filter for cloud cover less than 10%\n",
    ").item_collection()\n",
    "\n",
    "#how many matches?\n",
    "len(items)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5193cf1b-d9e2-4f78-998d-d38395de9324",
   "metadata": {},
   "source": [
    "Lets print some info about this Sentinel scene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082f0b0e-3b11-49e2-860d-4b90807329c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "items[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23f8ca0-0774-4c09-a466-419cd875a660",
   "metadata": {},
   "source": [
    "### 4. Load our data\n",
    "Now that we have the data we want we can load it into an xarray. We could pass the s3 url of the data direclty to xarray, but the package `stackstac` does a bunch of additional data handling to return a neat result with lots of helpful metadata. We can, for example, give `stackstac` a list of multiple Sentinel 2 scenes and it will align and stack them along a time dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d582a3-72d3-4553-ba4b-09135da8ec54",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stack = stackstac.stack(items[0],bounds=(bbox[0], bbox[1], bbox[2], bbox[3]))\n",
    "\n",
    "#stackstac create a time dim, but we only have 1 date so we drop this\n",
    "stack = stack.squeeze()\n",
    "\n",
    "#select only the bands we need\n",
    "stack = stack.sel(band=['blue', 'coastal', 'green', 'nir', 'nir08', 'nir09', 'red', 'rededge1', 'rededge2', 'rededge3', 'swir16', 'swir22'])\n",
    "\n",
    "#combine bands into one chunk\n",
    "stack = stack.chunk({'band':-1})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f537b4-6284-4d54-87d5-86ebce5471c9",
   "metadata": {},
   "source": [
    "Lets make a plot to see what it looks like in true color. We will use the package `hvplot` which makes it very easy to create interactive plots from xarrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd233627-53de-4eeb-a996-0f1ac02cd25a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stack.sel(band=['red','green','blue']).hvplot.rgb(\n",
    "    x='x', y='y', bands='band',rasterize=True,robust=True,data_aspect=1,title=\"True colour\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a9dd44-043f-42bd-93d2-88e6abfa1be6",
   "metadata": {},
   "source": [
    "What about a single band?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86bc2d7d-8990-4949-9656-f790dd1afc13",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stack.sel(band='red').hvplot(\n",
    "    x='x', y='y',rasterize=True,robust=True,data_aspect=1,cmap='magma',clim=(0,0.2),title='Red reflectance')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b84d80-124f-48c0-8a93-6dd5703de62a",
   "metadata": {},
   "source": [
    "#### Shadow Masking \n",
    "In the image above some areas are shadowed by mountains. It is unlikely that we will be able to predicct land cover in these areas, so lets mask them out. We will use a simple rule that says if the reflectance in the red and near-infrared is below a threshold, drop those pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3bcb54f9-3ae4-45e2-b5c7-1fb363c0682e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#select red and nir bands\n",
    "red = stack.sel(band='red')\n",
    "nir = stack.sel(band='nir')\n",
    "\n",
    "# Set the reflectance threshold\n",
    "threshold = 0.05\n",
    "\n",
    "# Create a shadow mask: identify dark pixels across all bands\n",
    "shadow_condition = (red < threshold) & (nir < threshold)\n",
    "\n",
    "# Set shadowed pixels to nodata\n",
    "stack = stack.where(~shadow_condition)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74cfee50-c370-4100-986a-2146793abf10",
   "metadata": {},
   "source": [
    "### 5. Extract Sentinel data at point locations\n",
    "Now we will extract the reflectance data at the locations where we have validated land cover. The package `xvec` makes this easy. It returns the result as a xarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c067291f-9c6c-4f3a-95e3-9ed68cdd072a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Extract points\n",
    "point = stack.xvec.extract_points(gdf['geometry'], x_coords=\"x\", y_coords=\"y\",index=True)\n",
    "point = point.swap_dims({'geometry': 'index'}).to_dataset(name='s2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2212455d-2d0b-45c7-94c1-d4e1be401455",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#lets actually run this and get the result\n",
    "with ProgressBar():\n",
    "    point = point.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77951e60-806d-4694-abe6-899b76167846",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b4c97b-d2e3-41f9-8566-d8f2603c58a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#drop points that contain nodata\n",
    "condition = point.s2.notnull().any(dim='band')\n",
    "\n",
    "# Apply the mask to keep only the valid indices\n",
    "point = point.where(condition, drop=True)\n",
    "point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8c6d41-e40a-470e-a30d-a8e70dfc78e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#add label from geopandas\n",
    "gxr =gdf[['class','group']].to_xarray()\n",
    "point = point.merge(gxr.astype(int),join='left')\n",
    "point"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e499832-5120-439b-90a0-e962f715837e",
   "metadata": {},
   "source": [
    "Lets select a single point and visualize the data we will be using to train our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caec9aa2-1a35-43c1-8fe0-19ae6fc3f9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pointp = point.isel(index=0)\n",
    "pointp['center_wavelength'] = pointp['center_wavelength'].astype(float)\n",
    "pointp['s2'].hvplot.scatter(x='center_wavelength',by='index',\n",
    "                                         color='green',ylim=(0,0.3),alpha=0.5,legend=False,title = \"Single point data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687ce23b-22a7-453a-b02e-b3b430224ead",
   "metadata": {},
   "source": [
    "### 6. Train ML model\n",
    "We will be using a model called xgboost. There are many, many different kinds of ML models. xgboost is a class of models called gradient boosted trees, related to random forests. When used for classification, random forests work by creating multiple decision trees, each trained on a random subset of the data and features, and then averaging their predictions to improve accuracy and reduce overfitting. Gradient boosted trees differ in that they build trees sequentially, with each new tree focusing on correcting the errors of the previous ones. This sequential approach allows xgboost to create highly accurate models by iteratively refining predictions and addressing the weaknesses of earlier trees."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc577929-ad00-486e-815c-6a4ddd152725",
   "metadata": {},
   "source": [
    "Our dataset has a label indicating which set (training or test), our data belong to. We wil use this to split it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "08b564fc-39dd-41a4-82c8-530e9d95d1e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#split into train and test\n",
    "dtrain = point.where(point['group']==1,drop=True)\n",
    "dtest = point.where(point['group']==2,drop=True)\n",
    "\n",
    "#create separte datasets for labels and features\n",
    "y_train = dtrain['class'].values.astype(int)\n",
    "y_test = dtest['class'].values.astype(int)\n",
    "X_train = dtrain['s2'].values.T\n",
    "X_test = dtest['s2'].values.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628bddcf-5cf7-4a05-a96e-759834023fa9",
   "metadata": {},
   "source": [
    "The steps we will go through to train the model are:\n",
    "\n",
    "First, we define the hyperparameter grid. Initially, we set up a comprehensive grid (param_grid) with multiple values for several hyperparameters of the XGBoost model.\n",
    "\n",
    "Next, we create an XGBoost classifier object using the XGBClassifier class from the XGBoost library.\n",
    "\n",
    "We then set up the RandomizedSearchCV object using our defined XGBoost model and the hyperparameter grid. RandomizedSearchCV allows us to perform a search over the specified hyperparameter values to find the optimal combination that results in the best model performance. We choose a 5-fold cross-validation strategy (cv=5), meaning we split our training data into five subsets to validate the model's performance across different data splits. We use accuracy as our scoring metric to evaluate the models.\n",
    "\n",
    "WE fit the RandomizedSearchCV object to our training data (X_train and y_train). This process involves training multiple models with different hyperparameter combinations and evaluating their performance using cross-validation. Our goal is to identify the set of hyperparameters that yields the highest accuracy.\n",
    "\n",
    "Once the search completes, we print out the best set of hyperparameters and the corresponding best score. The `random_search.best_params_` attribute provides the combination of hyperparameters that achieved the highest cross-validation accuracy, while the `random_search.best_score_ attribute` shows the corresponding accuracy score. Finally, we extract the best model (best_model) from the search results. This model is trained with the optimal hyperparameters and is ready for making predictions or further analysis in our classification task.\n",
    "\n",
    "This will take approx 5-20 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a71ec2b-c662-4a50-b919-a60d0ddec905",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'max_depth': [5, 9],         \n",
    "    'learning_rate': [0.1, 0.2], \n",
    "    'subsample': [0.6, 0.9],     \n",
    "    'n_estimators': [50, 200]    \n",
    "}\n",
    "\n",
    "# Create the XGBoost model object\n",
    "xgb_model = xgb.XGBClassifier(tree_method='hist')\n",
    "\n",
    "# Create the search object\n",
    "random_search = RandomizedSearchCV(\n",
    "    xgb_model,\n",
    "    param_grid,     \n",
    "    n_iter=10, \n",
    "    cv=5,\n",
    "    scoring='accuracy', \n",
    "    random_state=42, \n",
    "    n_jobs=-1)\n",
    "\n",
    "# Fit the search object to the training data\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best set of hyperparameters and the corresponding score\n",
    "print(\"Best set of hyperparameters: \", random_search.best_params_)\n",
    "print(\"Best score: \", random_search.best_score_)\n",
    "best_model = random_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d82332-3914-46f6-a759-d43962de7e4e",
   "metadata": {},
   "source": [
    "We will use our best model to predict the classes of the test data and calculate accuracy.\n",
    "\n",
    "Next, we assess how well the model performs for predicting Pine trees by calculating its precision and recall. Precision measures the accuracy of the positive predictions. It answers the question, \"Of all the instances we labeled as Pines, how many were actually Pines?\". Recall measures the model's ability to identify all actual positive instances. It answers the question, \"Of all the actual Pines, how many did we correctly identify?\". You may also be familiar with the terms Users' and Producers' Accuracy. Precision = User' Accuracy, and Recall = Producers' Accuracy.\n",
    "\n",
    "Finally, we create and display a confusion matrix to visualize the model's prediction accuracy across all classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b7dbec-570c-4433-adad-49217073847f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Step 2: Calculate acc and F1 score for the entire dataset\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {acc}\")\n",
    "\n",
    "# Step 3: Calculate precision and recall for Pine\n",
    "precision_pine = precision_score(y_test, y_pred, labels=[2], average='macro', zero_division=0)\n",
    "recall_pine = recall_score(y_test, y_pred, labels=[2], average='macro', zero_division=0)\n",
    "\n",
    "print(f\"Precision for Pines: {precision_pine}\")\n",
    "print(f\"Recall for Pines: {recall_pine}\")\n",
    "\n",
    "# Step 4: Plot the confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred,normalize='pred')\n",
    "\n",
    "ConfusionMatrixDisplay(confusion_matrix=conf_matrix,display_labels=list(name_table['name'])).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca7d032-3335-4221-92b9-54ff4c0118c0",
   "metadata": {},
   "source": [
    "\n",
    "### 7. Predict over entire study area\n",
    "We now have a trained model and are ready to deploy it to generate predictions across an entire Sentinel 2 scene and map the distribution of invasive plants. This involves handling a large volume of data, so we need to write the code to do this intelligently. We will accomplish this by applying the .predict() method of our trained model in parallel across the chunks of the sentinel 2 xarray. The model will receive one chunk at a time so that the data is not too large, but it will be able to perform this operation in parallel across multiple chunks, and therefore will not take too long.\n",
    "\n",
    "Our model was only trained on data covering natural vegetaiton in a specific area, It is important that we only predict in the areas that match our training data. We will therefore mask out non-natural vegetation using a polygon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ddafef11-9581-4766-bf4c-2b52bfa9f2d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "geodf = gpd.read_file('aoi.gpkg').to_crs(\"EPSG:32734\")\n",
    "geoms = geodf.geometry.apply(mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5551332-51f1-445b-a1ea-d645930af06a",
   "metadata": {},
   "source": [
    "Here is the function that we will actually apply to each chunk. Simple really. The hard work is getting the data into and out of this function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "83e2d93a-de7e-48b3-a4af-ddef6960e64c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict_on_chunk(chunk, model):\n",
    "    probabilities = model.predict_proba(chunk)\n",
    "    return probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e40270d-2a10-46cc-8808-0e15ad609bad",
   "metadata": {},
   "source": [
    "Now we define the funciton that takes as input the Sentinel 2 xarray and passes it to the predict function. This is composed of 3 parts:\n",
    "\n",
    "Part 1: Applies all the transformations that need to be done before the data goes to the model. It sets a condition to identify valid data points where reflectance values are greater than zero and the stacks the spatial dimensions (x and y) into a single dimension.\n",
    "\n",
    "Part 2: Applies the machine learning model to the normalized data in parallel, predicting class probabilities for each data point using xarray's apply_ufunc method. Most of the function invloves defining what to do with the dimensions of the old dataset and the new output\n",
    "\n",
    "Part 3: Unstacks the data to restore its original dimensions, sets spatial dimensions and coordinate reference system (CRS), clips the data, and transposes the data to match expected formats before returning the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3e5cff3a-867a-4bca-b1b5-4247b46eeca2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict_xr(ds,geometries):\n",
    "\n",
    "    #part 1 - data prep\n",
    "    #condition to use for masking no data later\n",
    "    condition = (ds > 0).any(dim='band')\n",
    "\n",
    "    #stack the data into a single dimension. This will be important for applying the model later\n",
    "    ds = ds.stack(sample=('x','y'))\n",
    "\n",
    "\n",
    "    #part 2 - apply the model over chunks\n",
    "    result = xr.apply_ufunc(\n",
    "        predict_on_chunk,\n",
    "        ds,\n",
    "        input_core_dims=[['band']],#input dim with features\n",
    "        output_core_dims=[['class']],  # name for the new output dim\n",
    "        exclude_dims=set(('band',)),  #dims to drop in result\n",
    "        output_sizes={'class': 10}, #length of the new dimension\n",
    "        output_dtypes=[np.float32],\n",
    "        dask=\"parallelized\",\n",
    "        kwargs={'model': best_model}\n",
    "    )\n",
    "\n",
    "    #part 3 - post-processing\n",
    "    result = result.unstack('sample') #remove the stack\n",
    "    result = result.rio.set_spatial_dims(x_dim='x',y_dim='y') #set the spatial dims\n",
    "    result = result.rio.write_crs(\"EPSG:32734\") #set the CRS\n",
    "    result = result.rio.clip(geometries).where(condition) #clip to the protected areas and no data\n",
    "    result = result.transpose('class', 'y', 'x') #transpose the data - rio expects it this way\n",
    "    return result.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5351b2d-d19e-414e-a0db-b5ccbf8ba08e",
   "metadata": {},
   "source": [
    "now we can actually run this. It should take about 30-60s (to go through an 10GB sentinel scene!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f9c35f-74be-4300-89c1-5a8921707b06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with ProgressBar():\n",
    "    predicted  = predict_xr(stack,geoms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ada586-1dcb-4e05-9084-ca1be0909a2c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb0bba3-788a-4015-bbe1-7d4ad0bd35c6",
   "metadata": {},
   "source": [
    "Now we can view our result. We will plot the probablity that a pixel is covered in invasive Pine trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8f6fb9bd-4239-4407-aa04-cb6de1f3d637",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#reproject\n",
    "predicted = predicted.rio.reproject(\"EPSG:4326\",nodata=np.nan)\n",
    "#select only pines\n",
    "predicted_plot = predicted.isel({'class':2})\n",
    "#set low propbability to na\n",
    "predicted_plot = predicted_plot.where(predicted_plot > 0.5, np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b09dbe-0de9-43ba-a9b6-e91f64e62237",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot with a satellite basemap\n",
    "predicted_plot.hvplot(tiles=hv.element.tiles.EsriImagery(), \n",
    "                              project=True,clim=(0,1),\n",
    "                              cmap='magma',frame_width=800,data_aspect=1,alpha=0.7,title='Pine probablity')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458cc122-c454-429b-af9a-41d8a8c0582c",
   "metadata": {},
   "source": [
    "Lastly, we export to a geotiff. We can use rioxarray to do this. Now we can explore the map in our desktop GIS if desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f080a383-da80-4e9c-8ae4-8838aaa625fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predicted.rio.to_raster('gctwf_invasive.tiff',driver=\"COG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c31fb8-2357-497a-9744-8d10a085c147",
   "metadata": {},
   "source": [
    "### credits:\n",
    "This lesson has borrowed heavily from the follwoing resrouces, which are also a great place to learn more about handling large geospatial data in python:\n",
    "\n",
    "[The Carpentries Geospatial Python lesson by Ryan Avery](https://carpentries-incubator.github.io/geospatial-python/)  \n",
    "\n",
    "[The xarray user guide](https://docs.xarray.dev/en/stable/user-guide/index.html) \n",
    "\n",
    "[An Introduction to Earth and Environmental Data Science](https://earth-env-data-science.github.io/intro.html)\n",
    "\n",
    "Another good place to start learning more is the [Cloud-Native Geospatial Foundation](https://cloudnativegeo.org/), which curates a community using and developing cloud-native geospatial tools"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda-geospatial-gathering:Python",
   "language": "python",
   "name": "conda-env-.conda-geospatial-gathering-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
